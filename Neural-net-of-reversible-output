Neural net of reversible output

Mar 19 2023 - Nikolay Valentinovich Repnitskiy - License: WTFPLv2+ (wtfpl.net)



I had discovered a neural network whose output can be reversed through that
network. Each layer is equal in neuron count--trivial, as unneeded output can
be ignored. Each layer contains values 1-n yet ordered randomly if you wish.
Training means swapping values in layers rather than changing those values.

C++ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Load input[n] with binary input, and create fire[n]. Layer 1 neuron 1 happens to
be 5,000  so the first 5,000 input[] elements are scanned for 1s.  The number of
1s found is modded 2. If 0, first element of fire[] is now equal to binary 0,
else binary 1.

Layer 1 neuron 2 happens to be 4 so the first 4 input[] elements are scanned for
1s. The number of 1s found is modded 2. If 0, second element of fire[] is now
equal to binary 0, else binary 1. Finish fire[]. Now move to layer 2 whose items
scan the first x elements of fire[] for 1s, mod by 2, and written to fire_2[n].
Finish fire_2[] then copy it to fire[].

For efficiency, alternate between fire[] & fire_2[] instead of copying.
You get the process. Modular arithmetic motivation is taken from Douglas R.
Hofstadter who likes the idea of fire cancellation; neuron receives a number of
inputs who are summed. If sum is even, no activation yet one more makes it go
off. And "not all neurons connected" is motivated by nature. Originally, neurons
were to increase value upon training--creating more connections. But reversible
output means each layer is 1-n permutation.

Reversing ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Find the location y of value 1 in the last layer and logically deduce: what
binary value in the previous fire[] first element could have caused a resulting
binary value fire_2[y]?

Find the location z of value 2 in the last layer and logically deduce: what
binary value in the previous fire[] first & second elements could have caused a
resulting binary value fire_2[z]?

That's a process. Moving chronologically while respecting order by corresponding
locations, WHILE modding sums of binary values as they grow in length by 1 gives
you deductive abilities.

Usefulness ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Not sure yet, but I got here first! Running through the model and pausing with
progress record, then running backward from the other end and meeting where you
left off running first, it's not always possible to rearrange that interjecting
layer to allow perfect passage. That's worth investigating because running in
from the end, the output you start with can be any desired classification or
generation. Other than that, any produced output run backwards will revert
to the input verbatim.

Last edited: Mar 20 2023 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
