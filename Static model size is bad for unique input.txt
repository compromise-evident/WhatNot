/* If you could magically feed a spider, it should remain motionless until death.
The spider brain--I will not even begin on what that structure looks like, the principles of its neuron
firing, memory of path retention, or how it got to that point. I just want to stress the importance of
constant stimulation from external sources--at least while the spider doesn't have the complexity to
mentally shelter and classify its brain as its new environment, from which it might learn independently.

Spiders have only a hundred thousand neurons, perhaps they fire one thought, and one though only as they
sit there on a web. The web vibrates, they feel that energy, and begin the search for flies stuck to the web.
If they're seriously hungry, they might move and set up webs where flies frequent the area, or they might
increase the size of their web as a last resort while making their home obvious to birds. These are all
motives pushed by external sources, whether it's the squeezing pouch to signal hunger or the need for air.

Perhaps a spider's brain is limited to function and reaction. Their brain needs not fire differently
unless sources external to the brain require so. Then IS the spider intelligent? From this perspective,
no. Its brain is a model dictating what to do for any possible stream of input-data.

Insect brains are small enough to attempt to re-create and run quickly. The grid below represents a brain,
boxes are individual neurons. This brain is a three-dimensional cluster meant to be imagined as though the
grid was printed on paper, crumpled down to a sphere, and interneural connections penetrate the paper for
line-of-sight firing. That's right, a double-index array paired with what I call computational imagination
gives you a an easy way to start. Probing one neuron then passing its information to another, necessarily
means there is a new connection between the two even if performed only once. Each neuron within a column
may affect any neuron within the next column to the right therefore you can say connected neurons are
spaced near one another in any configuration you prefer. And double-index arrays give you convenient
arithmetic methods to navigate through this "cerebral cortex" left to right, column by column.
As you can see, you can easily build your very own neural net right now.


    fire >        [a][ ][ ][ ][ ][ ][ ]
    fire >        [ ][ ][ ][ ][ ][ ][ ]
    fire >        [ ][ ][ ][ ][ ][ ][ ]                             Same reality,
    fire >        [ ][b][ ][ ][ ][ ][ ]                            new perspective
    fire >        [ ][ ][ ][ ][ ][ ][ ]
    fire >        [ ][c][ ][ ][ ][ ][ ]
    fire >        [ ][ ][ ][ ][ ][ ][ ]                                  [b]
    fire >        [ ][ ][ ][ ][ ][ ][ ]                             [f]       [c]
    fire >        [ ][ ][ ][ ][ ][ ][ ]                                  [a]
    fire >        [ ][d][ ][ ][ ][ ][ ]                              [e]     [d]
    fire >        [ ][ ][ ][ ][ ][ ][ ]
    fire >        [ ][e][ ][ ][ ][ ][ ]
    fire >        [ ][f][ ][ ][ ][ ][ ]


This model--the spider brain--modifies itself as was shown where butterflies avoided a certain color
after being shocked while sitting on one blob but not another. But just how many such life-saving memories
can these insects force on their model? If endlessly tortured, perhaps their model might one day reserve
a special region just for memories--at the cost of losing less-important functions due to the brain size
constraints. With this attitude, models should degrade as they're shown something high-profile or dramatically
different even just one time. Unless of course, your model is allowed to grow as when comparing newborn brains
to adult brains. But really, I only want an electronic spider to help me with my cryptanalysis needs.
To reiterate the title, the limited compute of a spider means it cannot experience or sense too many input
types and has automatic generalization and in turn categorization for input streams. By capping the size of
your model while feeding it unique inputs, the model may remember a new function at the cost of losing old
functions. Not only that, but if your model decides to reserve regions for memory at the cost of losing other
regions, it might over-generalize its input streams--take for example a Christmas detector that's only on when
it's Christmas. Not very useful. Your AI requires AGI to manage itself the way we do. But until the
singularity assigns part numbers to your partitions, this should do it.

With spider brain regions in mind, perhaps the next questions are, what would happen if model size is adjusted
too rapidly, and what is the optimal adjustment rate? What if like a newborn, our models should start small as
spiders do while slowly increasing their neuron count and exposure to all the problems we've created? Then would
we see distinct regions responsible for different psychological finctions become apparent on their own? */
